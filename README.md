ğŸŒ± Carbon Emissions of GPUs in LLM Training
ğŸ“Œ Project Overview

This project estimates the carbon footprint of GPUs used in AI / Large Language Model (LLM) training.
It combines GPU specifications, real-world power measurements, and global electricity carbon intensities to calculate the environmental impact of training AI models.

We answer:

How much COâ‚‚ is emitted when training an LLM on different GPUs?

How do GPU power ratings (TDP) affect emissions?

How do country-specific electricity mixes change results?

ğŸ“Š Datasets Used

GPU Specs Dataset (gpu_specs_v7.csv) â†’ memory, shaders, clocks, TDP

BUTTER-E Dataset (butter_e_energy.csv) â†’ real GPU power measurements

EU Carbon Intensity (2017_CO2_IntensEL_EEA.csv)

Global Energy Mix (OWID) (owid-energy.csv)

âš™ï¸ Methodology

GPU Power Estimation

Use TDP from dataset if available.

If missing, apply a heuristic based on shaders, clock speed, and memory size.

Energy Calculation

EnergyÂ (kWh)
=
Watts
Ã—
Hours
1000
EnergyÂ (kWh)=
1000
WattsÃ—Hours
	â€‹


Carbon Emission Calculation

COâ‚‚Â (kg)
=
EnergyÂ (kWh)
Ã—
CarbonÂ IntensityÂ (kgCOâ‚‚/kWh)
COâ‚‚Â (kg)=EnergyÂ (kWh)Ã—CarbonÂ IntensityÂ (kgCOâ‚‚/kWh)

Validation

Use BUTTER-E dataset to compute measured kWh and emissions.

Compare against estimated values.

LLM Training Simulation

Function simulate_llm_training() lets you input GPU watts, training hours, country, and year to estimate COâ‚‚ emissions.

ğŸ“ˆ Key Results

High-end GPUs like NVIDIA A100 or RTX 3090 emit the most COâ‚‚ when used continuously.

Emissions vary significantly by country depending on energy grid carbon intensity.

Example:

400W GPU running 24h in the US (2017): ~4.56 kg COâ‚‚

Same GPU in France (2017, low-carbon grid): ~0.74 kg COâ‚‚

Validated against real-world measurements from BUTTER-E dataset.

ğŸ› ï¸ Files Generated

gpu_emission_estimates_24h.csv â†’ estimated 24h emissions for all GPUs

butter_e_summary.csv â†’ measured emissions from BUTTER-E dataset

ğŸš€ How to Run

Open the Kaggle notebook.

Upload datasets (gpu_specs_v7.csv, butter_e_energy.csv, 2017_CO2_IntensEL_EEA.csv, owid-energy.csv).

Run all cells to:

Clean and estimate GPU power usage

Calculate COâ‚‚ emissions

Simulate LLM training emissions

Export results

ğŸŒ Why this matters

Training large AI models consumes massive energy. By quantifying emissions, this project helps raise awareness about the environmental cost of deep learning and encourages more carbon-aware AI research.
